{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Crash course 2: Convexity</center>\n",
    "### <center>Alfred Galichon (NYU & Sciences Po) and Giovanni Montanari (NYU)</center>\n",
    "## <center>'math+econ+code' masterclass on optimal transport and economic applications</center>\n",
    "<center>© 2018-2022 by Alfred Galichon. Past and present support from NSF grant DMS-1716489, ERC grant CoG-866274, and contributions by Jules Baudet, Pauline Corblet, Gregory Dannay, and James Nesbit are acknowledged.</center>\n",
    "\n",
    "#### <center>With python code examples</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex analysis\n",
    "\n",
    "## References\n",
    "* [OTME], Ch. 6\n",
    "* Rockafellar (1970). Convex analysis. Princeton.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental tool in convex analysis is called the Legendre-Fenchel transform, which is defined in general as follows.\n",
    "\n",
    "**Definition**. The Legendre-Fenchel transform of $u$ is defined by\n",
    "\n",
    "$$ u^{\\ast}\\left(  y\\right)  =\\sup_{x\\in\\mathbb{R}^{d}}\\left\\{  x^{\\intercal}y-u\\left(  x\\right)  \\right\\}  . $$\n",
    "\n",
    "\n",
    "\n",
    "**Proposition**. The following holds:<br>\n",
    "(i) $u^{∗}$ is convex.<br>\n",
    "(ii) $u_1\\leq u_2$ implies $u^*_1\\geq u^*_2$.<br>\n",
    "(iii) (Fenchel's inequality): $u(x)+u^{∗}(y) \\geq x^{⊺}y$.<br>\n",
    "(iv) $u^{∗∗} \\leq u$ with equality iff $u$ is convex.\n",
    "\n",
    "    \n",
    "As an immediate corollary of (iv), we get the fundamental result:\n",
    "\n",
    "**Proposition**. If u is convex, then $u=(u^{∗})^{∗}$. The converse holds true.\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some examples of Legendre-Fenchel transforms.\n",
    "\n",
    "One has:\n",
    "\n",
    "(i) For $u\\left(  x\\right)  =\\left\\vert x\\right\\vert ^{2}/2$, one gets\n",
    "$u^{\\ast}\\left(  y\\right)  =\\left\\vert y\\right\\vert ^{2}/2$.\n",
    "\n",
    "(ii) For $u\\left(  x\\right)  =\\sum_{i}\\lambda_{i}x_{i}^{2}/2$, $\\lambda_{i}>0$, one gets $u^{\\ast}\\left(  y\\right)  =\\sum_{i}\\lambda_{i}^{-1}y_{i}^{2}/2$.\n",
    "\n",
    "(iii) The entropy function\n",
    "\n",
    "$$u\\left(  x\\right)  =\n",
    "\\begin{cases}\n",
    "\\sum_{i=1}^{d}x_{i}\\ln x_{i}\\text{ for }x\\geq0\\text{, }\\sum_{i=1}^{d}x_{i}=1 \\\\\n",
    "+\\infty\\text{ otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "has a Legendre transform which is the log-partition function, a.k.a. logit\n",
    "function\n",
    "\n",
    "$$u^{\\ast}\\left(  y\\right)  =\\ln\\left(  \\sum_{i=1}^{d}e^{y_{i}}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subdifferentials\n",
    "\n",
    "\n",
    "We now restate the demand sets of workers and firms in terms of subdifferentials of convex functions. For this, let us recall the basic economic interpretation duality: $u^*(y)$ captures the problem of a firm of type $y$, which hires a worker $x$ who offers the best trade-off between production if hired by $y$ (that is $\\Phi\\left(  x,y\\right) =x^{\\intercal}y$) and wage $u\\left(  x\\right)  $. Thus, firm $y$ will be willing to match with any worker $x$ whithin the set of maximizers of $x^\\top y -u(x)$, while worker $x$ will be willing to match with any firm whithin the set of maximizers of $x^\\top y -u^*(y)$. \n",
    "\n",
    "These set of maximizers are called *subdifferentials* of $u^*$ and $u$.\n",
    "\n",
    "\n",
    "\n",
    "**Definition.** Let $u:\\mathbb{R}^{d}\\rightarrow \\mathbb{R}$. The subdifferential of $u$ at $x$, denoted $\\partial u(x)$, is the set of $y \\in R^{d}$ such that $\\forall \\tilde{x} \\in \\mathbb{R}^{d}, u(\\tilde{x})\\geq u(x)+y^{\\top}(\\tilde{x}-x)$.  \n",
    "\n",
    "The definition does _not_ require $u$ to be convex; however, if $u$ is convex, the previous Definition immediately implies that\n",
    "\n",
    "$$\\partial u(x) = argmax_{y} \\{ x^{⊺}y-u^{∗}(y) \\}$$\n",
    "\n",
    "hence the subdifferential of a convex function is always nonempty (while the subdifferential of a non-convex function can be empty in general).  \n",
    "\n",
    "When $u$ is differentiable and convex, then\n",
    "\n",
    "$$\\partial u(x) = \\{\\nabla u(x)\\}.$$ \n",
    "\n",
    "As an example, when $u\\left(  x\\right)  =\\left\\vert x\\right\\vert $, one has  $\\partial u\\left(  x\\right)  =\\left\\{  -1\\right\\}  $ if $x<0$, $\\left\\{  +1\\right\\} $ if $x>0$, and $\\left[  -1,+1\\right]  $ if $x=0$.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subdifferentials: first properties\n",
    "\n",
    "It also follows that if $u$ is a convex function, the following statements are\n",
    "equivalent:\n",
    "\n",
    "$$\\text{(i)}   \\text{ }u\\left(  x\\right)  +u^{\\ast}\\left(  y\\right)\n",
    "=x^{\\intercal}y$$\n",
    "$$\\text{(ii)}   \\quad y\\in\\partial u\\left(  x\\right) $$\n",
    "$$\\text{(iii)}  \\quad x\\in\\partial u^{\\ast}\\left(  y\\right).$$\n",
    "\n",
    "\n",
    "Going back to our worker-firm example, this has a straightforward economic\n",
    "interpretation. If worker $x$ chooses firm $y$, then $y$ maximizes\n",
    "$x^{\\intercal}\\tilde{y}-u^{\\ast}\\left(  \\tilde{y}\\right)  $ over $\\tilde{y}$,\n",
    "thus $y\\in\\partial u\\left(  x\\right)  $. This means that while worker $x$'s\n",
    "equilibrium wage $u\\left(  x\\right)  $ is in general greater or equal than the\n",
    "value $x^{\\intercal}y-u^{\\ast}\\left(  y\\right)  $ she can extract from firm\n",
    "$y$, those two values necessarily coincide if $x$ and $y$ are willing to\n",
    "match, in which case $u\\left(  x\\right)  +u^{\\ast}\\left(  y\\right)\n",
    "=x^{\\intercal}y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subdifferentials and complementary slackness\n",
    "\n",
    "These considerations allow us to relate the solutions to the primal and dual\n",
    "problems. Recall that in the finite-dimensional case, the primal and the\n",
    "dual problems are related by a complementary slackness condition. In the\n",
    "present case, let $\\left(  X,Y\\right)  \\sim\\pi$ be a solution to the primal\n",
    "problem, and $\\left(  u,u^{\\ast}\\right)  $ be a solution to the dual problem.\n",
    "Then almost surely $X$ and $Y$ are willing to match, which, by the previous\n",
    "discussion, implies that\n",
    "\n",
    "$$\n",
    "u\\left(  X\\right)  +u^{\\ast}\\left(  Y\\right)  =X^{\\intercal}Y,\n",
    "$$\n",
    "\n",
    "or equivalently $Y\\in\\partial u\\left(  X\\right)  $ or in turn $X\\in\\partial\n",
    "u^{\\ast}\\left(  Y\\right)  $. In other words, the support of $\\pi$ is included\n",
    "in the set $\\left\\{  \\left(  x,y\\right)  :u\\left(  x\\right)  +u^{\\ast}\\left(\n",
    "y\\right)  =x^{\\intercal}y\\right\\}  $. This condition appears as the correct\n",
    "generalization of the complementary slackness condition in the\n",
    "finite-dimensional case. Without surprise, taking the expectation with respect\n",
    "to $\\pi$ of the equality right above yields the equality between the value of\n",
    "the dual problem on the left-hand side, and the value of the primal problem on\n",
    "the right-hand side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of convex functions\n",
    "\n",
    "More can be said when $u$ is differentiable at $x$. In that case, it is not\n",
    "hard to show that $\\partial u(x)=\\left\\{ \\nabla u\\left(x\\right)\\right\\}$, i.e. contains only one point, which is $\\nabla u\\left(x\\right)  =\\left(  \\partial u\\left(  x\\right)  /\\partial x_{i}\\right)  _{i}$, the vector of partial derivatives of $u$, or gradient of $u$. Similarly, if $u^{\\ast}$ is differentiable at $y$, then $\\partial u^{\\ast}\\left(  y\\right)=\\left\\{  \\nabla u^{\\ast}\\left(  y\\right)  \\right\\}  $. Hence, if $u$ and $v$ are differentiable, then the equivalence mentioned in the first properties implies that $y=\\nabla u\\left(  x\\right)  $ if and only if $x=\\nabla u^{\\ast}\\left(  x\\right)  $, that is\n",
    "\n",
    "$$\n",
    "\\left(  \\nabla u\\right)  ^{-1}=\\nabla u^{\\ast}.\n",
    "$$\n",
    "\n",
    "\n",
    "Alternatively, this can be seen as a duality between first-order conditions and the envelope theorem. First order conditions in the firm's problem imply that if worker $x$ is chosen by firm $y$, then $\\nabla u\\left(  x\\right)  =y$, but the envelope theorem implies that the gradient in $y$ of the firm's indirect profit $u^{\\ast}\\left( y\\right)  $ is given by $\\nabla u^{\\ast}\\left(  y\\right)  =x$, where $x$ is chosen by $y$. Thus the first-order conditions and the envelope theorem are conjugate in the sense of convex analysis.\n",
    "\n",
    "As an example, when $u\\left(  x\\right)  =\\sum_{i}\\lambda_{i}x_{i}^{2}/2$, $\\lambda_{i}>0$, recall that $u^{\\ast}\\left(  y\\right)  =\\sum_{i}\\lambda_{i}^{-1}y_{i}^{2}/2$. Define $\\Lambda=diag\\left(  \\lambda\\right)  $. One has $\\nabla u\\left(x\\right)  =\\Lambda x$ and $\\nabla u^{\\ast}\\left(  y\\right)  =\\Lambda^{-1}y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessians of convex functions\n",
    "\n",
    "Assume both $u$ and $u^{\\ast}$ are stricly convex and differentiable. Then it\n",
    "can be show that their Hessians are invertible at all points, and that if\n",
    "$y=\\nabla u\\left( x\\right) $, then \n",
    "$$D^{2}u^{\\ast}\\left(  y\\right)  =\\left(  D^{2}u\\left(  x\\right)  \\right)\n",
    "^{-1}.$$\n",
    "\n",
    "This can be obtained by differentiating the relationship $\\nabla u^{\\ast}\\left(  y\\right)  =\\left(  \\nabla u\\right)  ^{-1}\\left(  y\\right)  $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**Exercise 1**  \n",
    "\n",
    "Compute the Legendre-Fenchel transforms of the following functions:\n",
    "\n",
    "(i) $u\\left(  x\\right)  x^{\\intercal}\\Sigma x/2$, where $\\Sigma$ is a positive definite matrix, one has $u^{\\ast}\\left(  y\\right)  =y^{\\intercal}\\Sigma^{-1}y/2$.  \n",
    "\n",
    "(ii) Let $p>1$ and $u\\left(x\\right)=\\frac{1}{p}\\left\\Vert x\\right\\Vert^{p}$, where $\\left\\Vert .\\right\\Vert $ is the Euclidean norm. Then $u^{\\ast}\\left(  y\\right)  =\\frac{1}{q}\\left\\Vert y\\right\\Vert ^{q}$, where $q>1$ such that $1/p+1/q=1$.  \n",
    "\n",
    "(iii) $u\\left(x\\right)  =1\\left\\{  x\\in\\left[  0,1\\right]  \\right\\}$.\n",
    "\n",
    "------\n",
    "\n",
    "**Exercise 2**  \n",
    "\n",
    "Give the subdifferentials of the following functions from $\\mathbb{R}$ to\n",
    "$\\mathbb{R}$:\n",
    "\n",
    "(a) $u\\left(  x\\right)  =\\max\\left(  x,0\\right)$.  \n",
    "\n",
    "(b) $u\\left(  x\\right)  =\\max\\left(  f\\left(  x\\right)  ,g\\left(  x\\right)\\right)  $, where both $f$ and $g$ are convex and differentiable.  \n",
    "\n",
    "(c) $u\\left(  x\\right)  =\\max_{1\\leq i\\leq n}\\left\\{  a_{i}x+b_{i}\\right\\}  $ where $a_{1}<a_{2}<...<a_{n}$.  \n",
    "\n",
    "(d) $u\\left(  x\\right)  =-x^{2}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### More on the entropy function\n",
    "\n",
    "Consider the entropy function\n",
    "$$\n",
    "u\\left(  x\\right) =\\begin{cases}\n",
    "\\sum_{i=1}^{d}x_{i}\\ln x_{i}\\text{ for }x\\geq0\\text{, }\\sum_{i=1}^{d}x_{i}=1\\\\\n",
    "+\\infty\\text{ otherwise}%\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As it is defined on the simplex, it is not a differentiable function from\n",
    "$\\mathbb{R}^{d}$ to $\\mathbb{R}$. Instead, let us take $x_{d}=1-\\sum_{i=1}^{d-1}x_{i}$, and let us view $u$ as a function $\\tilde{u}$ from $\\mathbb{R}^{d-1}$ to $\\mathbb{R}$. We define\n",
    "\n",
    "$$\n",
    "\\tilde{u}\\left(  x\\right)  =\\sum_{i=1}^{d-1}x_{i}\\ln x_{i}+\\left(1-\\sum_{i=1}^{d-1}x_{i}\\right)  \\ln\\left(  1-\\sum_{i=1}^{d-1}x_{i}\\right)\n",
    "$$\n",
    "\n",
    "if $x\\geq0$, $\\sum_{i=1}^{d-1}x_{i}\\leq1$, $\\tilde{u}\\left(  x\\right)=+\\infty$ otherwise.\n",
    "\n",
    "**Exercise 3**\n",
    "\n",
    "Show that:\n",
    "\n",
    "(a) The Legendre transform of $\\tilde{u}$ is a function of $\\mathbb{R}^{d-1}$\n",
    "to $\\mathbb{R}$ given by\n",
    "$$\n",
    "\\tilde{u}^{\\ast}\\left(  y\\right)  =\\ln\\left(  \\sum_{i=1}^{d-1}e^{y_{i}%\n",
    "}+1\\right)  .\n",
    "$$\n",
    "\n",
    "(b) The gradient of $\\tilde{u}$ is a vector in $\\mathbb{R}^{d-1}$ given by\n",
    "$$\n",
    "\\nabla\\tilde{u}\\left(  x\\right)  =\\left(  \\ln\\left(  \\frac{x_{i}}{1-\\sum\n",
    "_{i=1}^{d-1}x_{i}}\\right)  \\right)  _{1\\leq i\\leq d-1}%\n",
    "$$\n",
    "\n",
    "(c) The gradient of $\\tilde{u}^{\\ast}$ is a vector in $\\mathbb{R}^{d-1}$ given\n",
    "by\n",
    "$$\n",
    "\\nabla\\tilde{u}^{\\ast}\\left(  y\\right)  =\\left(  \\frac{e^{y_{i}}}{\\sum\n",
    "_{i=1}^{d-1}e^{y_{i}}+1}\\right)  _{1\\leq i\\leq d-1}%\n",
    "$$\n",
    "\n",
    "(d) Compute $D^{2}\\tilde{u}$ and $D^{2}\\tilde{u}^{\\ast}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of constrained optimization\n",
    "\n",
    "### The Karush-Kuhn-Tucker theorem\n",
    "\n",
    "Source: Gordon and Tibshirani's lectures: https://www.cs.cmu.edu/\\symbol{126}ggordon/10725-F12/slides/16-kkt.pdf\n",
    "\n",
    "\n",
    "- Consider the \"primal\" problem\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^{n}}  &  f\\left(  x\\right) \\\\\n",
    "s.t.~  &  h_{i}\\left(  x\\right)  \\leq0,i=1,...,m\\\\\n",
    "&  l_{j}\\left(  x\\right)  =0,j=1,...,r\n",
    "\\end{align*}$$ \n",
    "where the functions $f$ and $h_{i}$ ($1\\leq i\\leq m$) are convex, and $l_{j}$ ($1\\leq j\\leq r$) are affine.\n",
    "\n",
    "- This problem can be written as\n",
    "$$\\min_{x\\in\\mathbb{R}^{n}}\\max_{u_{i}\\geq0,v_{j}}\\underset{L\\left( x,u,v\\right)  }{\\underbrace{f\\left(  x\\right)  +\\sum_{i=1}^{m}u_{i} h_{i}\\left(  x\\right)  +\\sum_{j=1}^{r}v_{j}l_{j}\\left(  x\\right)  }}\n",
    "$$ \n",
    "where $L\\left(  x,u,v\\right)  $ is the Lagrangian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duality\n",
    "\n",
    "- In general the weak duality inequality\n",
    "$$\n",
    "\\min_{x}\\max_{w}L\\left(  x,w\\right)  \\geq\\max_{w}\\min_{x}L\\left(  x,w\\right)\n",
    "$$\n",
    "holds. Equality (strong duality) requires Slater's condition: there is $x_{0}$\n",
    "with $h_{i}\\left(  x_{0}\\right)  <0$ for all $i$ and $g_{j}\\left(x_{0}\\right)  =0$ for all $j$.\n",
    "\n",
    "- The \"dual\" problem is thus\n",
    "$$\n",
    "\\max_{u_{i}\\geq0,v_{j}}g\\left(  u,v\\right)\n",
    "$$\n",
    "where $g\\left(  u,v\\right)  :=\\min_{x}L\\left(  x,u,v\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Karush-Kuhn-Tucker (KKT) conditions\n",
    "\n",
    "- A pair $\\left(  x,u,v\\right)  $ satisfies the KKT conditions iff and\n",
    "only if\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&  0\\in\\partial f\\left(  x\\right)  +\\sum_{i=1}^{m}u_{i}\\partial h_{i}\\left(\n",
    "x\\right)  +\\sum_{j=1}^{r}v_{j}\\partial l_{j}\\left(  x\\right) \\\\\n",
    "&  u_{i}^{\\top}h_{i}\\left(  x\\right)  =0~\\forall i\\\\\n",
    "&  h_{i}\\left(  x\\right)  \\leq0~\\forall i,~l_{j}\\left(  x\\right)  =0~\\forall\n",
    "j\\\\\n",
    "&  u_{i}\\geq0~\\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- In practice, we shall use these conditions when $f$ and all the $h_{i}$ are smooth, in which case the condition rewrites as \n",
    "\n",
    "$$\n",
    "\\nabla f\\left(  x\\right)  +\\sum_{i=1}^{m}u_{i}\\nabla h_{i}\\left(  x\\right)\n",
    "+\\sum_{j=1}^{r}v_{j}\\nabla l_{j}\\left(  x\\right)  =0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to the **KKT Theorem**.\n",
    "\n",
    "In the setting above, the following two statements are equivalent:\n",
    "\n",
    "\n",
    "- $x^{\\ast}$ is optimal for the primal problem, and $\\left(  u^{\\ast},v^{\\ast}\\right)  $ is optimal for the dual problem,\n",
    "- $\\left(  x^{\\ast},u^{\\ast},v^{\\ast}\\right)  $ satisfies the KKT conditions.\n",
    "\n",
    "\n",
    "Note that the KKT theorem can be stated under more general assumptions(relaxing convexity assumptions on $f$ and $h$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
